## 什么是随机梯度下降

### sigmoid函数
 
远离中心位置的点,5层神经网络之后其梯度就会退化为0


### ReLU函数
容易出现死亡问题,一次更新不当,之后将再被激活

### w的更新

$$w' = w+\eta\delta\frac{df_1(e)}{de}x_1$$

### 梯度消失

$\delta'(x)的最大值为\frac{1}{4}$,而我们初始化的网络权重$|w|$通常都小于1,因此$|\delta'(z)w|\leq \frac{1}{4}$,对于链式求导,层数越多,$\frac{\partial c}{\partial b_1}$求导结果越小,因而导致梯度消失的情况发生.

### 梯度爆炸

$|\delta'(z)w|>1$,也就是w比较大的情况.但是对于使用sigmoid激活函数来说,这种情况比较少.因为$\delta'(z)$的大小与$w$有关$(z=wx+b)$,除非该层的输入值$x$在一直一个比较小的范围内.
