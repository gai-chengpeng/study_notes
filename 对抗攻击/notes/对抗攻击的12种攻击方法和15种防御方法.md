
论文：Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey

论文地址：https://arxiv.org/abs/1801.00553

| 类型 | 思路 | 批注 |
| --- | --- | --- |
| 研究背景 | 本文的主要内容是什么？目前研究情况是什么？| a |
| 方法和性质 | 研究对象是什么？作者如何采集数据？这项研究是在何时何地进行的? | a |
| 研究结果 | 有哪些亮点？有哪些惊喜？| a |
| 数据 | 表格、图表和插图最引人注目的是？为什么作者要包含它们? | a |
| 结论 | 作者从中学到了什么? | a|
| 研究展望 | 对未来的研究有什么暗示或建议? | a |
| 重要性 |为什么这项研究很重要?| a |
| 想法和问题 | 你有什么想法和问题？ | a |
| 本文好的表达摘录 | 能复用的要点是什么？ | a |

尽管深度学习在很多计算机视觉领域的任务上表现出色，Szegedy et al. [22] 第一次发现了深度神经网络在图像分类领域存在有意思的弱点。他们证明尽管有很高的正确率，现代深度网络是非常容易受到对抗样本的攻击的。这些对抗样本仅有很轻微的扰动，以至于人类视觉系统无法察觉这种扰动（图片看起来几乎一样）。这样的攻击会导致神经网络完全改变它对图片的分类。此外，同样的图片扰动可以欺骗好多网络分类器。这类现象的深远意义吸引了好多研究员在对抗攻击和深度学习安全性领域的研究。

自从有了 Szegedy 的发现，机器视觉领域中陆续出现了好几个有意思的受对抗攻击影响的结果。例如，除了在特定图像的对抗性扰动之外，Moosavi-Dezfooli et al. [16] 展示了「通用扰动（universal perturbations）」的存在（如图 1 所示），这种通用扰动可以让一个分类器对所有图片错误分类。同样的，Athalye et al. [65] 展示了即使用 3D 打印的真实世界中存在的物体也可以欺骗深度网络分类器（如图 2 所示）。考虑到深度学习研究在计算机视觉的重要性和在真实生活中的潜在应用，这篇文章首次展示了在对抗攻击领域的综合考察。这篇文章是为了比机器视觉更广泛的社区而写的，假设了读者只有基本的深度学习和图像处理知识。不管怎样，这里也为感兴趣的读者讨论了有重要贡献的技术细节。

第 2 节里列举了机器视觉中关于对抗攻击的常用术语

第 3 节回顾了针对图片分类任务的对抗攻击。

第 4 节单独介绍了在实际生活场景中对抗攻击的方法。

第 5 节关注对抗攻击的工作焦点和研究方向。

第 6 节讨论了防御对抗攻击的文献。

第 7 节以讨论过的文献为基础的展望了未来的研究方向。

第 8 节总结并画上结尾。