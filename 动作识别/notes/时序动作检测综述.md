# Temporal Action Detection (时序动作检测)综述

最近几年由于网络上视频量的急剧增多和神经网络的飞快发展,这项任务得到了更多的关注.目前这项任务的主要数据集有THUMOS2014、ActivityNet.评价指标为IOU,目前2017的大多数工作在IOU=0.5的情况下达到了20%-30%的MAP,虽然较2016年提升了10%左右,但是在IOU=0.7时直接降低到了10%以下,2018年IOU=0.5有34%的MAP.

目前的趋势是寻找视频内活动的相关性来更精准的定位,寻找可以代替光流的方法来加速模型.本文对近两年时序动作检测的主要工作进行了一个简单的综述.

# 一、任务
时序动作检测主要解决的是两个任务:localization+recognization

- 1)where:什么时候发生动作,即开始和结束时间；
- 2)what:每段动作是什么类别

一般把这个任务叫做Temporal Action Detection,有的直接叫Action Detection,还有叫Action Localization、

# 二:评价指标:

## 1.AR-AN(Average Recall vs. Average Number of Proposals per Video):

Temporal Action Proposal任务不需要对活动分类,只需要找出proposals,所以判断找的temporal proposals全不全就可以测评方法好坏,常用Average Recall vs. Average Number of Proposals per Video (AR-AN) 即曲线下的面积(ActivityNet Challenge 2019就用这个测评此项任务).

- AR: 平均召回率,AR = 所有视频(在某个或多个tIOU阈值下)的召回之和 / 视频数
- AN: 平均每段视频提交的proposals数目,$AN_submission = \frac{total\ number\ of\ proposals\ in\ the submission\ file}{total\ number\ of\ videos\ in\ the testing\ subset}$
 
```
说明:
1)AN其实就是每段视频你提交的proposal个数
2)比如设置ANmax = 100,则AN坐标轴每个格子表示提交1个到100个proposals,对应AR坐标轴为提交x个proposal时候的Average Recall;
3)需要注意的是如果找到的proposals不够AN数值则复制最低分的proposals到满足数量要求,如果找到的proposals超过AN数值则只要分数高的那些.
4)一般需要计算 tIOU 从0.5到0.9 并以step为0.05的AR, 再计算AR-AN. 
简单来说就是:在某个IOU阈值下(或者多个IOU阈值的平均),AR是平均召回,AN是提交的proposals数目(不够就复制分最高的,多了就删掉分低的),AR-AN就是提交AN个proposals时候的AR,连成曲线,求出曲线下面积就是Activatinet的评价指标.
```
【更详细的信息以及CODE请参考】: http://activity-net.org/challenges/2019/tasks/anet_proposals.html

## 2.mean Average Precision (mAP):

Temporal Action Detection(Localization)问题中最常用的评估指标.一般对tIOU=0.5(或tIOU在(0.5, 0.9]上步长0.05的均值)进行对比,tIOU是时间上的交并.

```
IoU:
目标检测中loU(交并比)是模型所预测的检测框和真实(ground truth)的检测框的交集和并集之间的比例,动作检测改为对时间(维度是1)的IoU.
以下计算都需要指定在某个或多个IOU阈值下的,比如IOU = 0.5的时候的P、AP	
Precision:
计算出每个视频中每个类的正确检测次数(A),给定一个视频的类别C的Precision=本视频正确预测(True Positives)的类别C的proposal数量 / 本视频在类别C的所有proposal总数量
AveragePrecision:
Precision计算的是一段视频一个类别的精度,但是我们的测试集有很多视频,所有视频在某类C的平均精度=在测试集上所有的视频对于类C的精度值的和/有类C这个目标的所有视频的数量.
MeanAveragePrecision:
AveragePrecision计算的是所有视频对于某个类别的平均精度,但是我们有很多类别,MAP=所有类别的平均精度和 / 所有类别的视频数目
Average MeanAveragePrecision:
不同tIOU阈值下的map均值,如计算 tIOU 从0.5到0.9 并以step为0.05的map均值作为指标.
简单来说就是:在某个IOU阈值下,P是一段视频中预测proposal在一个类别的精度,AP是所有视频的proposal在某个类别的P的均值,map是所有类别的AP的均值
```
【更详细的信息以及CODE请参考】: http://activity-net.org/challenges/2019/tasks/anet_localization.html

# 三、DataSet:
## 1.THUMOS2014
该数据集包括行为识别和时序行为检测两个任务,大多数论文都在此数据集评估.

- 训练集:UCF101数据集,101类动作,共13320段分割好的视频片段；
- 验证集:1010个未分割过的视频；其中200个视频有时序行为标注(3007个行为片 段,只有20类,可用于时序动作检测任务)
- 测试集:1574个未分割过的视频；其中213个视频有时序行为标注(3358个行为片段,只有20类,可用于时序动作检测任务)

## 2.ActivityNet
200类,每类100段未分割视频,平均每段视频发生1.54个行为,共648小时

## 3.MUTITHUMOS
一个稠密、多类别、逐帧标注的视频数据集,包括30小时的400段视频,65个行为类别38,690个标注,平均每帧1.5个label,每个视频10.5个行为分类,算是加强版THUMOS,目前我只在Learning Latent Super-Events to Detect Multiple Activities in Videos这篇论文看到了该数据集的评估.

## 建议:
如果刚开始看这方面,17工作直接看SSN（TAG找proposal）、R-C3D、CBR（TURN找proposal）就好了,找proposal方法简单看看TAG和TURN（网络其他部分不用看）,github也有代码,对性能要求不高可以试试SSN（用到了光流）,不然的话可以用一下R-C3D.

SSN代码:https://github.com/yjxiong/action-detection

CDC代码:https://github.com/ColumbiaDVMM/CDC

R-C3D代码:https://github.com/VisionLearningGroup/R-C3D

CBR代码:https://github.com/jiyanggao/CBR

Learning Latent Super-Events to Detect Multiple Activities in Videos
代码:https://github.com/piergiaj/super-events-cvpr18

# 四、基本流程
1. 先找proposal,在对proposal分类和回归边界
2. 找proposal方法:主要就是以下几种
- (1)单纯的滑动窗口（SCNN提出）:固定一些尺寸在视频长度上滑窗,重叠度越高,效果越好,但是计算量大.理论上这种方法只要重叠度够高,是找的最全的,但是冗余多.
- (2)时序动作分组（TAG提出）:逐个视频帧分类（CNN网络）,把相邻的类别一样的分成一组,设置一些阈值防止噪声干扰,一般设置多组阈值防止漏掉proposal.这种方法对于边界比较灵活,但是可能会因为分类错误漏掉proposal.
- (3)单元回归（TURN提出）:把视频分成固定大小单元,比如16视频帧一组,每组学一个特征（放C3D里）,然后每组或者多组作为中心anchor单元（参照faster-rcnn）向两端扩展找不同长度proposal.

# 五、目前的主要方法